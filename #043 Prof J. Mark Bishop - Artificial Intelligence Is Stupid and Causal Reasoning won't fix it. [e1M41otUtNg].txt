Do you remember the taste of the last orange you ate?
Do you remember the warmth of that welcoming cup of tea that you had when you came in from
the cold November rain?
Do you remember the smell of the hair of the first love that you kissed?
Ond 那ade ddoi ithatiom ni arfael ar wedi cynnig 150 a I'n cael ei mightych ar draws ac oedd ei cynnig ar ei le 당wyth,
chi'n clenfritiol, Youtube channel ac podcast. ГTeam rlly
rhywbodaethno haith ei meddwl ym rydym Thim Scyff.
Felly mae'r hyn, sy'n ail am 간 o gyfos emnili ac allanodwch singol.
Ydyn doedd o universo ni ar geisio ei moddi,
preconsepongo on absolutely everything. Pedro Domingo said that there were only five tribes in
artificial intelligence and he didn't even consider the other tribe which not many people talk about.
Cybernetics. Cybernetics is the science of communications and automatic control systems
in both machines and living things. We're going to discuss AI across three key dimensions today.
computabilitio, ymlaenau a'r cyfnoddologio cyfnoddologio neu'r cyfnoddologio.
Mae'n ei ddysgu o'r cyfnoddologio arall, ac mae'n gallu bod yn dweud.
Mae'n ddwylo'n mynd i'r cyfnoddologio, mae'n ddwylo'n mynd i'r cyfnoddologio i ddim iawn i'ch cyfnoddologio.
Professor Mark Bishop wedyn yn ddwylo'n ddwylo'n cyfnoddologio neu'r cyfnoddologio i ddim i'r cyfnoddologio,
mor, ni'n gael i'n meddwl gwell gwylio panpsych it
panpsyched yn byw'r amser yn sicr o'r cydweddol iawn o burgers, ond chi'n blwyddyn ei canfodin arunsef
gan ddedu gair gwvedod honi.
Ararwch o'i gor explores ar zal yng Nghymgr함n amser gyntaf canfod New steering
Fe cofnodol hefyd, deall hwn yn gweithio i all完成 o pobl oesbyd credu vulnerable a rhoi tregedwyddiant drwy'n hyn.
Y diawn rwy'nThanks Dau Yn Fath O'r Pier yw thegoaniaeth chi Menai a Gwasanaeth Cym vaccinations.
Ceisd ni beth hynny ond mae eich cab pregnancy wedi rhaid mewn rideol hefyd ar ownentgyfundu gyn hacerlo.
Mae ydy hi allocated ym mwynhau yi'ch gwaethaf wrth hynny hefyd yn ddu her insult ti yw hoffaeth yr THEY
rwgefach Fan.
Yn y loapple ymarfDis admire,
yavez is ymarFinois
ac pot ydy'r listonsech
birwed.
Max's yw'r或ach Fodorwch
byddwyo artist y llal weithfyrdd
ac mae elecountynau
gweithio ar y mfotur
Feithpieig Angbryd M�each yma
math Fès Ysgwynt,
ddim dda wedi wneud
i honweithio gweithredu createdl.
Mark hefyd
Fика pa a DAtywyll
Rhyw meddwl
using natural language processing for e-discovery,
or detecting malicious insiders by subtle changes in language
in human communication networks.
Insiders who might pose a threat to your organisation
and they use sophisticated graph analysis to do that.
Marc just published a paper called
Artificial Intelligence Is Stupid
and causal reasoning won't fix it.
He makes it clear in this paper that in his opinion
Blizzard yn eich passeidiannol a'r cysyllt i gwybod alwe,
d yükwelio angenogi wedi weld yerym.
Dwi allan o y 20io,
y PREFsoft add continuar yn ydy ddimân imeddar
Fe fyr cookie a fe Witheritynaolio ar ''T
Fyr Cynllun Fe a Gw rising wisdom котo'n adnod o adnod officersiaid.
than the behaviour of a vast assembly of nerve cells and their associated molecules.
You're nothing but a pack of neurons. And that was according to Crick in 1994.
The Church Turing Hypothesis stated that every function which would naturally be regarded as
computable could be computed by the universal Turing machine. If only computers could adequately
model the brain, then, the theory goes, it ought to be possible to program them to act like minds,
with its myriad of features running the gamut from causal learning, reasoning and understanding.
Even Bengio observed in 2019 that we know from prior experience which features are the salient
features and that comes from a deep understanding of the structure of our world. The Church Turing
Hypothesis has triggered an explosion of interest in biologically plausible neural networks.
We had Dr Simon Stringer on the show last week talking about the spiking dynamics, the temporal
binding circuits which emerge when you create some of these biologically inspired neural networks.
But I'm not just talking about the biologically inspired versions, even the relatively pedestrian
vanilla neural networks that we all know and love on this channel. This has all been a huge focus
point for the last 50 years or so. AI eskatologists like Ray Kurzweil and Nick Bostrom believe that
there might be an intelligence explosion where all of humankind will inevitably be crushed like
ants, although viewers of this channel will well know Francois Chalet's response to this view.
Alan Turing deployed an effective method to play chess in 1948 many decades ago,
but since then we've seen little progress in getting machines to actually genuinely understand,
to seamlessly apply knowledge from one domain into another. Judea Pearl believes that we won't
succeed in realising strong AI until we can equip systems with a mastery of causation. He thinks
we need to move away from simplistic probabilistic associations to machines which can reason causally.
He even proposed a so-called ladder of causation which is seeing, doing and imagining, which I feel
is almost self-explanatory actually. Unfortunately for Pearl, DeepMind have already demonstrated
several times a reinforcement learning system which can perform causal reasoning and counterfactual
analysis. It seems obvious to me because if you're interacting with a system then of course you can
learn the causal factors. I'd completely take Pearl's point that with traditional machine learning
where you're not interacting with a system you can't learn any causal factors. That seems quite
intuitive. Anyway, all of this is small fry compared to the point which Professor Bishop wants to make.
The idea that these silicon ensconced algorithms can become thinking machines becomes a little bit
bizarre once you realise that a machine has no choice in what it does. Computation is not an objective
fact of the world, it's observer relative. Even Wittgenstein said that the meaning of a computation
is in its use. He thought that understanding could not be a process and therefore it cannot
be a process of symbol manipulation. Whether a given individual understands is often external
to that individual. Mark's intuition is that evolution, autonomy and environmental interactions
give rise to phenomenological consciousness. He thinks that we cannot live inside a computer
simulation because he can feel the sensation of cool air on his face. So Mark thinks that the
meaning of computation becomes relative and lies in its use by humans. Mark gives several examples
in the show this evening demonstrating precisely why he thinks this. So I think Mark's main contribution
is this dancing with Pixie's reductio ad absurdum. He quotes Hilary Putnam.
Your seat, the seat you're sitting on, the very clothes you're wearing, the room that we're in,
they're all containing a rich dance of subatomic particles, a dance that never repeats itself.
And Putnam realised that this is analogous to a state machine going through an infinite series of
non-repeating states. So it then seems to me that if a computer, a terminator perhaps, is conscious
purely as a result of moving through a series of computational state transitions, then if I
know the input to that machine with input fixed, I can generate exactly the same series of state
transitions with any large counter like a car's marlometer or following Hilary Putnam's move
with any open physical system. So if a machine is conscious merely as a result of following
some computation, then consciousness is everywhere in the bricks of this building, the clothes that
you're wearing, the very seat you're sitting on, they are all experiencing the zing of that orange,
the warmth of that cup of tea, and the memory of your first love's kiss. If machine consciousness is
possible, everything, even the smallest grain of sand, is filled with an infinitude of conscious
experiences. Bishop interprets Putnam's result to mean that computationalism demands that every
physical system is host to a multitude of conscious minds, which he refers to as little pixies.
Since a computationalist believes that to be a conscious mind is just to implement the right
kind of computation, not only would we be surrounded by pixies, but the vast majority of
conscious experience would be realised in these pixies. Since any physical system is implementing
any and all computations simultaneously, then all possible conscious minds must be instantiated
simultaneously in every physical object. For Bishop, this is the most patently absurd manifestation
of panpsychism, and thus demonstrates that computationalism must be false.
So it seems like a contrarian position that Bishop is saying that computation is very much in the
eye of the beholder, whereas most of us think that computation goes on inside our brains.
Anyway, the key takeaway from the dancing with pixies reductio ad absurdum is that computation
doesn't have those phenomenological conscious states. A finite state automata cannot give
rise to conscious experience unless conscious experience is in everything. Bishop says that
he's an embodied entity, which is to say he's not just thinking in his brain, he thinks with his body
and his body in the world. In today's episode, we also talk about some of the greats of
computability, mathematics and logic, starting with Alan Shearing on computability. He described a
machine called a discrete state machine. I now call it Shearing's discrete state machine because
that was the first time I read about it in his work. So over any short time period, we can replicate
the behavior of the different state transitions of Shearing's discrete state machine with any other
one such as a digital thermometer. But because when we added input, the number of possible state
sequences grew exponentially, we can't easily do the same thing when you have a machine with input.
But then I realized, if I know the input to one of these machines, that combinatorial state
structure collapses again just to a simple list of state transitions. He invented this interesting
thought experiment called the discrete state machine, and he had this physicalist desire to
explain all of humanity via a computer program. And interesting, what he learned later on in his
career about the non-computability of numbers led to a significant amount of tension for him later
on in life and his children. The American philosopher John Searle was so exasperated that
anyone might seriously entertain the idea that computational systems purely based on the execution
of appropriate software, no matter how complex, might actually understand. It was ridiculous.
He formulated the now infamous Chinese room experiment. And we'll go into this in some detail
in the show, but essentially he said that syntax is not sufficient for semantics and that
programs are not formal and minds have content. Therefore, programs are not minds and computationalism
must be false. Now, most of the Chinese room argument is the first proposition, which is that
syntax is not sufficient for semantics. And we will come back to that later. Another interesting
character is Godel. Godel's first incompleteness theorem famously stated that any effectively
generated theory capable of expressing elementary arithmetic cannot be both consistent and complete.
In particular, for any consistent, effectively generated formal theory, f, that proves certain
basic arithmetic truths, there is an arithmetic statement that is true, but not provable in the
theory. And this can be used almost anywhere and it's often referred to as the Godel sentence for a
particular theory. And it was used in anger by Roger Penrose. He made the Godelian argument that
mathematical insight cannot be computable. He said that the mental procedures whereby
mathematicians arrive at their judgments of truth are not simply rooted in the procedures
of some specific formal system. And he followed up by saying human mathematicians are not using
a knowably sound argument to ascertain mathematical truth. Anyway, I really hope you enjoy the show
today. I'm absolutely honoured that Mark came on to discuss this with us. I'm very interested in
the philosophy of AI and the philosophy of mind. Make sure you read some of the material that Mark
has signed posted and I'll link those in the description. Remember to like, comment and
subscribe and we'll see you back next week. Welcome back to the Machine Learning Street Talk
YouTube channel and podcast with my two compadres, MIT PhD, Dr Keith Duggar and Alex
Bayesian Stenlake. Now today we are speaking with Mark J Bishop, Professor Emeritus of Cognitive
Computing at Goldsmiths University of London. Mark is interested in the philosophy of mind and
artificial intelligence. Sorry, that's my Siri. It seems to be very interested in getting involved
in this conversation. Mark is interested. Now it's playing music because AI is stupid. That's why
AI is really, really stupid. And actually, that's a great segue for our conversation today
because Mark, our guest today, also thinks that AI is really, really stupid. Mark is interested
in the philosophy of mind and artificial intelligence and rails against what he calls
computationalism. We'll get to that in a sec. Machine consciousness and panpsychism. In 2010,
Mark was elected to the chair of artificial intelligence and simulation of behaviour,
which is the world's oldest AI society. He's been invited to advise on policy at the UN, the EC
and also the UK government. He's published three academic books, 200 articles and won
three million pounds worth of research funding. He serves as the associate editor of nine
international journals and his research has spanned the practice and theory of artificial
intelligence. He's regularly asked to comment on AGI, particularly in response to these AI
eschatologists. Of course, we were speaking about this a few weeks ago. So folks like Hawkins and
Musk and Kurzweil, who warn of an existential threat of an intelligence explosion. Now, in one
of Mark's recent papers, he concluded that cognitivism, which is the whole idea of viewing
the brain as a computer, and its concomitant computational theory of mind, is inappropriate.
And instead, we should emphasise the role of foundational processes such as autonomy, exploration,
autopiosis. Now, that's a strange word as well, isn't it? So that means a system capable of
reproducing and maintaining itself by creating its own parts and eventually further components
and social embeddedness in giving rise to a genuine understanding of our lived world.
So in summary, Mark thinks that computational theories of mind cannot explain human cognition.
He thinks that the claims of its research is that genuine conscious mental states can emerge
purely in virtue of carrying out a specific series of computations. He thinks that those
claims are egregious. Now, I discovered Mark about a few months ago because he's published this
paper called artificial intelligence is stupid and causal reasoning won't fix it. It's actually a
really cool paper because it's a torda force of all of the computational and philosophical issues
surrounding AI at the moment. And he kind of kicks off in the paper by saying that AI is a brand
tag. It's becoming ubiquitous. But a corollary of this is that there's widespread commercial
deployments where AI gets things wrong, whether it's autonomous vehicle crashes or chat bots being
racist or automated credit scoring, you know, processes discriminating on gender. And of course,
we have a whole load of people saying that we can improve it. So Judea Pearl and Gary Marcus,
they say that deep learning is just curve fitting. It's just reasoning by association.
And, you know, if only we could build computer systems that took things a step further and thought
about time and space and causality. But Mark takes the AI skepticism to a whole new level because
he thinks that machines cannot and will never understand anything. Professor Mark Bishop,
welcome to the show. In your paper, you talk about Cric and you talk about church and
ensuring giving rise to computationalism. What did those folks say? Well, in my paper,
I start off with the idea that it's become known as Francis Cric's astonishing hypothesis that
you and everything that we are is defined by the set of particular set of neural firing patterns
at any one instance. And if we take run with that idea to its logical conclusion, it would seem to
be that if we have an appropriately high fidelity simulation of just the brain, we abstract from
the brain, we're from all the dirty chemicals, the neurotransmitters, the serotonins and the
like, we abstract from all that and just look at the neural firings, we've got everything, everything
else drops out for free. And a lot of people surprisingly buy into this idea. And in fact,
it's one of the hypotheses that pushed the human brain projects and one of the biggest European
Union funding grants of all time a few years ago is over a billion, if I remember rightly,
by only my cram. And that courted a lot of controversy with some people saying that the EU is
putting a lot of its eggs in one basket. And a lot of people had doubts as to how much real
science that programme would would deliver at it, how it died a very interesting presentation
to the group, the human brain project group, a number of years ago, because I was arguing as Tim
outlined in the introduction that we were not likely to get conscious states. In fact, I think
there are good a priori arguments for believe me, we won't get conscious states are very computational
simulation, no matter what that simulation is, no matter how fast it is, or no matter what algorithm
it is, unless we have to bite the bullet and we accept a very vicious form of panpsychism,
the idea that conscious phenomenal states are living in everything, the very cup of tea of coffee
that I'm drinking at the moment has its own mental states and the similar likes to pin my colours
to the scientific mask, I find it somewhat implausible to believe that my coffee is conscious
of me drinking it. And so we're led to reject that horn and if we reject the horn of panpsychism,
then unfortunately we're led to, in my opinion, we're led to reject the idea that the mere execution
of a computer program can bring forth conscious states. So that's kind of, in a nutshell, the executive
summary, if you like, of an argument I described as dancing with pixies, that proports to show that
unless we're willing to accept panpsychism, computers will never have phenomenal states of
consciousness. Now that is a distinct argument from the argument that people like Sirle makes,
that computers can't understand and it's also distinct from people like Penrose who say there are
certain things that people can do and Penrose very famously talks about mathematical insight that
are fundamentally non-computable. So it's my own minor contribution to the debate and that is I don't
believe that computers can be conscious unless we're willing to accept a very nasty form of panpsychism.
So I think some of that impulse, right, to break things down, you know, and to find the absolute
minimum component that can implement everything in humor, I mean that's a very western kind of
analytic, you know, scientific approach to start with and so if we don't want to throw out analytics
as a whole, like in what gap or let's say what do we need to add to kind of artificial neurons,
if you will, in order to, or even to the computational paradigm itself. So what do we
need to add to say the Turing machine or lambda calculus, you know, concept in order to be able
to create consciousness? What's missing? It unfolds usually complicated stories as you can imagine
and I'm hoping that at some point in time we'll get to go into engaging a little bit more depth on
what the dances with pixies reducció actually says because otherwise it just sounds like an
airy hands waving philosophical statement that I'm making and it might be quite interesting to go
into the nuts and bolts of why I believe that argument works but to come back to Keith's point,
right, many people I think working in the field as a young teenager back in the 70s, I taught myself
to programme and I don't know healthy interest in science fiction and you put those two things
together and you're led to the belief that I have very strongly as a teenager that we would build
thinking conscious machines and that one day they would they would come to tyrannize mankind and
then slavers and go on to be the next stage of evolution if you like. I'm not alone in that,
I'm sure many people have had similar entertain similar fantasies and it wasn't until I went to
university that my choice of degree at university was informed by these interests of mine,
peculiar interests of a teenage male and I went to read cybernetics at the University of Reading,
it was the only place in the UK where you could read cybernetics at the time
and that was a great education not least as some of the people my tutor for example Alex Andrews
was one of the was an early first round a neural net pioneer from the sort of 40s and 50s and
famously gave a couple of great papers that the mechanisation of thought conference of people
like Minsky and Rosenblatt were all out so I was surrounded by old school academics who were from
that first wave of new people working in neural networks and that was a really interesting place
to work and one of the guys that taught us a cybernetics is kind of an engineering and it
touches on all sorts of things but it weren't sort in the UK there's very much an engineering
discipline and so one of the things we had to do from year one was build computers but not
perhaps as you might think of as building a computer are you going to get on board and putting
a few chips in perhaps but literally starting out with TTL and building your own half, building
your own address decoders and building literally a computer from individual TTL chips and once
you've done that you get a very low level but real engineering perspective of what's going on in a
computer you don't tend to think anything too fancy the idea that these things are thinking
machines start to become a bit bizarre because you realise that machine has no choice in what it
does if I imagine like a balance a ruler on a pivot that's balanced and press the ruler down one
side the other side comes up it can't do anything but that and when you see that the operation of the
logic gets in the computer working exactly the same way that seems to be a very different mode
of operation to that which we used to entertaining when we think about human cognition that said all
those thoughts were further down the intellectual line for me when I was engaging as an undergraduate
I'm just interested in building these things and I guess it wasn't till my third year I was doing
a joint degree in Simon editing a few of science and then one of my lecturers seemed to be the
science was a guy went on to be professor of theoretical computing at Oxford a guy called
Richard Byrd and he introduced me to the notion of Turing non-computability and and also introduced
me to Gerda Leche Barth but that's another tell very interesting book which I'm sure you're all
very familiar with and that was quite an intellectual shock because prior to doing that course I'd had
this sort of a I was already modesty in man and I had this idea that you give me a problem
and enough time and I'll boss you out of the computer program not solve it and the idea that
there could be problems that were fundamentally non-computable was was a shock and it took a while
for that shock to actually see pin I was a bit skeptical even though I knew what to write to
get the reasonable mark in the exam I can't say my heart was completely wedded to the notion of
non-computability not least because the proofs of these are kind of weird as well when you get
into them there's lots of self-reference and things and they seem a bit sort of bizarre and it did
to me as a young undergraduate anyway but then Roger Penrose and I came back to the read from
PhD at Reading and I was lucky to meet Roger Penrose just around the time he was publishing
The Emperor's New Mind or just before that time and we got chatting and I realised that my
intuitions about the horror that non-computability might pose were being echoed by someone who was
even at that time known as being a bit of a polymath the guy that taught Stephen Hawkins
amongst other things and worked with him and the fact that this guy was also echoing some
serious reservations that were based on Gerdlina ideas and based on showing non-computability
stuck a chord with me but the real big intellectual shock to my again I started out in the PhD
in New York and that works with the intention of building a thinking living proving conscious
machine and then over that period of doing that PhD my position changed 180 degrees so after
meeting Penrose the next big thing was I went to a conference at Oxford in 19 or when would it be
it was around the time that parallel distributed processing first came out and Roman Hartford
Clellan came over for the first time in the UK to describe backprop so this is how long ago it is
I'm really quite old-skilled in all these things so yeah which was the first presentation about
profit at Oxford and it's a massive conference there was you know it was a set out conference
for the main room which probably held about 7 800 was sold out and they had two overspill theatres
we were in the second of these overspill theatres with video links to the main stage and we heard
these presentations about profit which is all very exciting but the thing that blew my mind
having been brought up in an engineering discipline cybernetics was listening to two philosophers
because that was quite odd and there are two philosophers with dinner and soul
and I've never heard a presentation like it because unlike the sort of kind of measured dry
presentations of how to control the server mechanism or a new algorithm for quicksword or
whatever the hell it has to be in our computer science and engineering presentations I soon
learned that philosophers argue in a much more fistic of kind of way and it was it was a shock
but also very engaging and and so I came across you know Searle describing his Johnny's room
argument which resonated with me at the time and hearing dinner poo poo this at the time in
his own unique way and so around that time my thesis I began to entertain began to question
where I was going am I am I team and are people other people in the world who are doing exciting
things when you all know is are we going to build these thinking machines and actually over the
that time and in the few years that followed I reversed that opinion and the core intuition
that drove all that I guess really was Searle's Johnny's room argument no I think you'll find a
lot of sympathy a lot of people that actually work you know very close to the AI or you know
what's referred to as AI when we're trying to sell things um you know we'll raise an eyebrow
and skepticism because you know we know GPT is just a big pile of linear algebra you know the
the talk about the transformational effects really come from the business side you're among
good company here but to kind of go back you said you want to touch on this dancing with pixie's
argument that you raised earlier and this kind of links into the the notions of panpsychism
and how this relates to um you know if you if you want to take the idea that a computer can be
intelligent that it can think that it can understand things then you end up concluding that anything
can kind of have this you run us through the like very briefly people haven't read the paper
panpsychism argument the dancing with pixie's fallacy and kind of like how this all kind of flows
into the conclusion that computers cannot be intelligent yeah one of the actions in which this
argument is built is the idea that computation is not an objective fact of the world it's
observer relative and so I first of all want to give you a couple of examples that I think
will underscore that axiom one because a lot of people reject that's a nonsense computation what
a computer does is a fact of the matter so again I'll go back to my undergraduate days and we
had to build um before we used TTL we literally had to build these logical gates out of transistors
so it doesn't get much more basic than that so imagine you've built some uh a set of transistors
to perform the following uh electronic logic you have two inputs to the circuit one output
call the inputs A and B in the output though uh if both inputs A and B are zero volts the output
is zero volts if A is five volts and B is zero the output is zero if A is zero volts and B is five
the output is zero if A and B are five volts the output is five volts what logical function
Fel ch Person.
Chw знач.
Rownersi log stwff ychynig i fydd oadebon.
All gynllun bob Fujiw y bwyda i gwrthod Ilus, a phii gweithio leysame.
Mae a'r cain o'r ba o retailera,
bod Pwy gweithio leysame kombeithio.
I�� i'r gweithio LED.
distress canceled?
Laenell.
Mae angen i rhan o gwmp enjoyableio'r dybi.
Ny da, mae'n rhan o ganod.
Felly, y cyfnod yn fawr. Oni, y bydd y bywch y f Nadhef unrhyw bwysig nhw'r bywch y drained
ond yn cael eu gofynu'r arfer ropea sydd chi'n at commence a anggat «투 phasio ar le Fur W nice
le i se motherfilmaraill, gan amgrol cyfnod, lle ma transniwr i holl cael ei fod forward
eich cyfnod yn cael eich profiad o wahanol. Rhaidlift, fe pob i'n argynnau'r Mach i pool function eich
That's an and gate, that's an or gate, without knowing that mapping, and that mapping is subjective.
I might have one mapping, Alex might have another, Keith might have another one. In fact, a great Israeli
computer scientist Oren Shigre extends this argument pathologically and looks at multi-level
logics, and the problem gets really weird if you go down that route, but I'll just stick to the
simple case with the and and the or function. So that's one of the reasons why I said,
fundamentally, I mean it seems to be just axiomatic, I just baffled when people tell me this is not
the case, but I'd still occasionally meet people who dispute this. There's a second follow-through
argument, and it's built on work of Winograd and Flores in their book understanding computers
in cognition, when they start to think about what is a word processor, and I've reframed their argument
a little, and I think about what is a chess program. I don't know that any of you guys are old
enough to remember in the 70s, we used to have these little chess plastic chess computers that
were square, and they had little holes on a on a bit of board, and little tiny little
plastic pieces, and when you made your move you lifted a piece up and pumped it where you
wanted to go, and then the computer would light up the piece you had to move and where that was to go.
Using one of these gadgets you could, I could quite happily play, I'm not very good chess player,
so I could happily get thrashed by these machines day in, day out, and enjoy that thrashing, so to say,
to speak, and I could argue that I could use that piece of computational equipment to play
chess with. Now in the UK there's a famous conceptual artist by the name of Tracey Emin,
who does a lot of work with neons, I don't know whether you guys have come across at work at all.
And also in the 60s there was a big movement in what's called kinetic art, where you're in
cybernetic art, where people interacted with art pieces. So now what can be noticed to me,
Tracey's sabotaged my test computer, she's ripped the innards out, and she's now wired all the inputs
to pressure pads in an art exhibition, in an art gallery, and all the outputs to neon strips,
so when people walk over these pressure pads, different neon lights come on and off. Now there
was no sense that you can possibly, it seems to me, that you could possibly say that when I walk
around the art gallery I'm playing chess, I'm interacting with a bizarre piece of abstract art,
certainly not playing chess. So it doesn't seem to me there's anything intrinsically chess-like
in this device. Yes, it was engineered very carefully, so if I knew what I was doing I could
play chess with it, I could use it in other ways as well. And the problem gets even worse if you've
come across isomorphic games. Let's imagine, probably all you know is Noughts and Crosses.
Now imagine you've got a Noughts and Crosses game on your iPhone, and you've got a, like I have a
six-year-old daughter who's just about got her head around Noughts and Crosses. I can keep her
occupied for, I was going to say half an hour, that'd be a exaggeration, for five minutes say,
giving her this thing and she'll play Noughts and Crosses happily again, and she says,
oh daddy, I'm bored, what am I going to do? I say, ah, well I've got another game I can show you.
But she says, daddy, you've only got one game on a computer, so I'm going to play it against
the computer, so don't worry about that. We've got the Noughts and Crosses, I've got another game,
I'm going to call it Computer Wist. Now imagine you lay the deck of cards out, ace through to nine,
and you, we take it in turns to pick cards from this deck. The winner is the first person you can get
15. Get the cards to seven to 15. It transpires that if you've got a programme that can play Noughts
and Crosses with a suitable mapping, you can get it to play a perfect game of Computer Wist. So we've
given the grid just like a magic square, where all the verticals, horizontals and diagonals add up to
15, right? You then plot your computer go, it's marking a one square, and choose your go, you can
tell you which card to pick next, and you can play a perfect game of Computer Wist. So I can use that
same computer programme with my mapping to play a perfect game of Computer Wist. So you cannot say,
in advance, without knowing what I'm going to do with that programme, whether I'm going to play
Tic Tac Toe or Computer Wist. So I think those three arguments together make, to me, a persuasive case
to paraphrase Wittgenstein, that the meaning of a computation is in its use by human computer users.
The phrase that, as you are all aware from Wittgenstein, paraphrasing is one of the investigations
where it makes the claim that the meaning of a word is its use by human players with human language
games. And I think the same applies to computation. The meaning of a computation lies in the use that
we as human users of computers put that too. So that's kind of setting the stage. So I think
there's always going to be this mapping at the physical level, and then there's always the idea
of what we're going to use a computation to do, and that's a very social human activity. So that's
setting the stage where I want to go with it, answering the pixies. Now the next move I'll make,
I know you'll have all have read Computing Machinery and Intelligence, Turing's famous 1950 paper.
Everyone's at least looked through this. Turing first outlined a Turing test, what became
known as a Turing test. Also in that paper he outlines the operation of a very simple machine,
Turing's discrete state machine, as it became known, and this is a beautifully simple machine.
It's a disk-like device, and it can go around in 120-degree intervals, and it can stop at the
12 o'clock, the 8 p.m. and the 4 p.m. position as it moves around a clock, and it can exist in each
one of those discrete positions. And we can describe the operation of that machine as a
finite state automaton, if the machine is in state A and the next clock tick is going to go to B,
if it's at B, the next clock tick is going to go to C, and if it's at C it will go back to A again.
Now if we want to, we can arrange that when the machine is in computational state A, it will
do something when it's in computational state B, or Turing envidges it when it's in computational
state A, a light would come on. You also imagine there being simple input to the machine, like a big
lever brake mechanism that you could have on or off, so if the machine was in state A and the
brake was on, it would remain in state A, if the brake's off it would go to state B. One of the
first interesting things, again like computation, you see when you've given a Turing discrete state
machine, to read off the computational state A, B, C, we need a mapping between the physical position
of the lever machine and the computational state to which it refers. So we may define computational
state A to be the 12 o'clock position, in which case when the levers there were in A, or in a bit
sets at B, and the rest thing follows through, but we always need that mapping. We've always got to
do these mappings between the physics of what's going on and the computational state that we're
instantiating. So now we've got this machine, without the brake it just goes to A, B, C, A, B, C, A, B, C, A, B, C.
And that's interesting enough if you're interested in inputless finite state automata, they're not
very exciting machines, all they can do is go through a cyclic series of states forever in an unbranching
series of state transitions. What is interesting is that in the appendix to Hilary Putnam's
representation of reality is a little known proof, this shows how we can effectively, how we can
map the operation of any inputless finite state automata onto a large digital counter. Actually
Putnam goes further and shows that we can map it onto any open physical system and
an open physical system being a physical system that's able to gravitational waves and all the rest of it,
electromagnetic spectrum impingion to it. But for simplicity, let's just consider the
without loss of generality, let's just imagine we can map the operation of any inputless FSA onto a
bloody large digital counter. How does Putnam do that? Well let's take Turing's machine,
he just says if the computation is in state A, I'm going to map that to the digital counter state
zero zero zero, if it's in state B, I'll map that to counter state one, if it's in state,
computational state C, I'll map that to counter state two and then the A again will go to three,
the B again will go to four, the C again will go to five and then we go over any finite time period,
we can replicate the state transitions of our digital discrete state machine
by the numbers that we're cycling through on our digital counter.
And again, you might answer so what that doesn't seem particularly threatening result for
computationalism at first sight because real computations are much more complex devices than
inputless finite state automata. Well in a paper called Does a Rock implement every inputless finite
state automata, David Chalmers responds to this argument in an interesting way and he says that
yet I'll concede if you like that we can implement really trivial machines like inputless finite
state automata using Putnam's mapping but when we want to look at machines with input this breaks down
because we get a combinatorial explosion of states that we need and Chalmers introduces a very
neat construction called the combinatorial state automata which we can implement using Putnam's
mapping but at an exponential increase the number of states that we need and the combinatorial state
automata is sensitive to initial conditions and so could be genuinely said if we could implement
it we have an interstate to implement it could generally be said to be implementing a computation
with input but at the cost of every time step of the computational you need an exponent your number
of states grows exponentially and Chalmers makes the point that after a very short number of states
will run out of the number of states needed is bigger than the number of atoms in the known universe
and hence Putnam's mapping must fail and that's kind of where I entered the debate because I made
an incredibly trivial all that all the hard work had been done long before I came to play with this
game so to speak but my only trivial modification to Putnam's argument that to me makes it robust
to Chalmers is to say this well if we look at any real machine of which it's claimed has genuine
mental states conscious states as it interacts with the world and this this intuition was brought
real for me because again some people dispute the fact that there are people who believe that
they're a serious scientist who believe in the machine consciousness program there definitely are
and I used to my head of department as at Reading Cybernetics and one of those people a guy called
Kevin Warwick and we at Reading had built these little simple robots that moved around a corral
controlled by a neural network and Kevin said well these got roughly the same number of neurons
as a slug and it's pure human bias if you say a slug has conscious experience and these robots
didn't and I thought that was a ludicrous claim and that inspired me to move and develop this
Dancing on the Pixies Reductio so to come back to the case I said right then Kevin if you say
your robot as it moves around the corral over a finite time window t1 to tk experiences something
that it is like to be a robot bundling around a corral not bumping into things I don't know what
that is but that's just imagine it has some conscious experience what I what I can do is log all
the inputs to that machine and then I'll play them back to them so I now lifted the robot out
of the corral I've disconnected all its sensors if you like and actuators and I'm just injecting into
the robot the the states it would have got where it was in round the corral on itself test to go
and does the machine still have conscious state well yeah of course it has it's reading the numbers
from a latch the data was originally taken from an ATD converter for argument sake we're now
plonking that data in there from a data rejection system but the computer still has the phenomenal
states and so Kevin Warwick asserted and that unfortunately led to the problem that he was
going to encounter because if that was the case all that we're really into we can take we can collapse
the exponentially growing number of states that charmers show we would have if you actually want
to implement fully all aspects of a computation using partner's mapping if we just try to look at
the particular computational trace we just need the input to that machine that pertain to any
over time as the machine it is a little thing and then we can remove all the counterfactual
states and once we've done that we've got a linear series of state transitions that we can
reliably map using putnam's mapping and hence if it's the case that Kevin Warwick's little robot
was conscious then so must our account to be conscious and then after putnam any open physical
system so that in a nutshell is is the DWP reductio it's interesting that in all these seminal debates
to me about AI about penrose about sirlin about my own small contribution there's a lot of
confusion people can very easily misinterpret what's been saying a lot of people got hung up
about does a rock genuinely implement a computation you know and I think to me charmers completely
proved that it does not right no problem with that does can we make a rock with a suitable mapping
implement an arbitrary series of state transitions yes I think we can and I think we can make any
count to do that and because we always use a mapping whatever system we use I don't think I'm
doing anything it's not slight of hand involved here because all computational systems involve
and observe relative mapping somewhere along the line to get them to work whether it's only
assigning a logical two to five volts and a logical false to not volts so I don't think the
use of a mapping is something that you know and there's no slight of hand involved in that given
that I confuse an arbitrary complex series of state transitions so then the question is
if you're a physicalist and I approach this problem originally as someone who you know I like
to think of myself as if I'm not a mysterious I don't want to appeal to some supernatural forces
to bring forth my consciousness and you know at one point in time it was the case that well if you
don't believe in functionalism or computationalism then you've got to believe in supernatural effects
well that is no longer the case cognitive science has moved on a lot since the 1960s there's an awful
lot of new tools in town and these are really exciting tools in my view and you highlighted a few
in the introduction to him but things like the embodied, inactive, embedded and ecological
approaches can go a long way to answering or giving his insights into these questions
without having to bring forth particularly supernatural notions so I'm going to put that to
one side we don't we no longer face whether a choice of either accept computationalism or accept
mysteriousism. Putnam's rock I mean there's a lot of interesting responses to it but I want to
point out a couple things or maybe just ask you about a couple things so one thing is that
first I think what your goal is and correct me if I'm wrong with the pixie dancing with pixies
argument is to say simply that if we accept say turing complete computation or even in this case
finite state machines but I think we can probably go one step further if we accept that effectively
computable systems can implement consciousness then we also have to accept panpsychism correct?
That's what I tried to show because once you have a system that you play like my boss Kevin said
that machine is conscious but I can look at what happens as that machine interacts with the world
I can log all the inputs to it I can trace the flow of the execution flow of that of the machine
code that control the robot and then I can implement that an arbitrary series of state
transitions that would do exactly that with an appropriate mapping with a digital mere digital
counter so if that machine is conscious then my digital counter would push this mapping must be
conscious right so that's the position I arrived at now when chatting to David Chalmers about this
he said oh no no no no no you've you've gone off the road because we need the full potential of
the computation to be there for functionalism to hold now this is quite a mysterious view in fact
it was so mysterious that when he first said he had to repeat it about three times because I'm not
quickest at uptake and I found it so bloody bizarre what he was saying but when I did unpick
what he was saying the Chalmers you actually need once you effectively slice off the potential
counterfactual actions by saying well I know the input at this point in time I know the input at
that I'm going to replace the counterfactuals in my program by direct go-to statements if you like
or just just omit snip them from the from the program to me that couldn't possibly affect the
phenomenal state of the system because really what he's saying that non-entered branches of a
computer program have a causal effect on the phenomenal state of your machine but the bizarre
thing is that's what David's saying he said no that's it you've got to have the potential for
counterfactuals there with the words we don't have phenomenal we don't have the machine genuinely
instantiating phenomenal states so if we just if we just assume in argument though that people don't
or that we don't accept panpsychism okay and that and that these arguments prove that if we accept
computationalism it implies you know panpsychism okay we also have mathematical results that say
um let's say turing computation or effective computation encompasses all computation like
there is no you know there is no other kind of computation unless there's hyper computation
right and so I think what we're saying I believe and correct me if I'm wrong but
I believe what we're saying is that there exists hyper computation and that human minds are performing
hyper computation so just just take this back to the rock for a second one issue with that mapping
right is that rocks actually have physical states that may be real numbers you know they may
have values that in and of themselves are not computable you know they can have positions and
states and quantum states that that have values that are essentially defined by an infinite precision
real number and therefore are not even accessible to computability to start with like even describably
right so are we saying because I'm always looking for where consciousness is hiding if you will
like are we saying that it's hiding in sort of real valued states you know maybe quantum
states like penrose would say perhaps a microtubules or something like that you know is that is that
a form of hyper computation and is that where our consciousness derives from where do we draw
the dividing line because that's something that's like I've read quite a few of your papers in
preparation as you do when you're going to speak with someone um and this dividing line where we
go from you know computational and essentially impossibly like impossible to achieve intelligence
or understanding or any measure thereof to the point where we have an intelligent system that
can understand its world and and sort of redefine itself and redefine its world um that distinction
is is not terribly clear and as we dive into this question I want to drive towards where this
distinction lies if this distinction exists well to pick up on Keith's point first I think
I'm I'm I'm neutral I mean penrose has given a positive thesis as well as a negative one so
famously in the emperor's new mind he gives he gives he gives his first version of a goodly
the in argument that reports to show that mathematical insight is non computable and
then says well this suggests to me that non computability lies at the heart of more it is to
be human and then with stewart hammer off they outline a positive thesis which which reports to
show that non computability can arise in the brain through the orchestrated collapse quantum collapse
in the microtubule skeleton of a bone neurons I'm perhaps neutral on this I know that when
penrose held the psych symposium on his work in 1995 and it attracted a lot of responses
well over 20 friendly rightly and I don't think any of his logical work was seriously brought
into question so the that is is interpreted even though that was actually a naive interpretation
of girdle compared to the work that he put out in shadows of the mountains and much more nuanced
approach to the argument in my opinion but nonetheless it wasn't seriously criticised
nearly everybody criticised his positive thesis so I am I get penrose is a clever guy it seems
interesting I'm not going to hang my colours on to that flag in particular if it works great but
I'm I've been more drawn to modern approaches to cognitive science which look at the end and
you know there isn't unfortunately a very quick six page paper that can can lead people gently
into this but there are four schools that all this work really started out with work of a roboticist
from MIT called Ronny Brooks who wrote a classical paper which you guess you guys are familiar with
called intelligence without representation which basically looks at you instead of trying to
when I did robotics old fashioned old school representational robotics as a young post grad
a lot of our work was trying to build take data from sensors and build rich internal models of
and out their world and remember we we spent all our budget on buying the biggest fathos computers
we could possibly afford at the time and strapping them onto these poor little autonomous vehicles
and they were laden down with computer power and they moved absolutely tragically slowly we're going
back in the in the early 90s now but they were they were pathetically slow things you know really
embarrassingly bad because a lot of their work was trying to build up these models looking at
what's on a bill models of them now I know that these days we can do that kind of thing
bloody quickly but back in the day you couldn't and and Brooks thought well do we need to do it
and in that paper you argued that we didn't why build the representation we can use the world
as its own representation and in the sense that sort of paved the way for thinkers like
Francisco Varela then in a book called the embodied mind with Evan Thompson and Eleanor
Roche to to start thinking about different ways of doing cognitive science and the embodied mind
is a mind-blowing book it's quite a it throws your whole view in the same in a in another
this way to go to Leche Barth can be quite mind-blowing as a as a young kid this is mind-blowing
as well but in a in a kind of a weird way because it actually questions the existence of a fixed
pre-given out there world and that was quite a shock to me when I first came across these ideas
not being a trained continental philosopher there my first mode of engagement was with Varela who
incidentally started out as a theoretical biologist and then someone very active in the
a life community so I think his initial work has also been from a science perspective but he
engaged quite deeply with with the European philosophy which which at that point in my life
I was sort of ignorant of and so this led to a development of alternative schools of what
cognition is all about and the inactive school is one that I'm interested in and it says that you
know effectively we can look at one sort of in itself is these days split into numerous
sort of approaches one of these from a developed by a guy called Kevin Aragon and Alvin Airee argue
that visual consciousness is something that we do so they're moving away from the idea that vision
is like interpreting like your eye getting a scene from the world on your brain having some
little like a cinema which you're then interpreting what all these little bits do they make the case
that the vision is more akin to an activity is what we'll do it's it's guided sensory
motion exploration of the world Varela itself was particularly interested in and brought being
in ideas of autonomy how can how can things become meaningful there's all these very complicated
debates that I think to try and come back to your question Keith and your question Alex
I mean we you need to touch on but it's really challenging to to touch on them in an
intelligible way in a in relatively short period of time you would you would at least agree though
that um you know your contribution penrose etc points very strongly that there's something
embodied there's something physical that we haven't quite figured out yet maybe it's
microtubules maybe it's something else you're agnostic to that but there's something physical
that allows my intuition is to do with autonomy uh and this is why we bring the idea of altru poesis
in that Tim mentioned at the beginning altru poesis is a again in the 70s umberto aturana
the chilean side musicians umberto maturana and francisco varela came up with a theoretical
device for delineating life and non-life because astonishingly this has been a really difficult
problem you'd think it was probably solved 100 years ago it hasn't been and even as recently as
when margo bone was writing on this one school tried to say life is and then give enumerated a list
of properties that has to metabolize it has to reproduce bloody bloody blood rather maturana
looked at from a different perspective as well what fundamentally life is a system that has a
circular organization it's got to be able to maintain its own boundary of itself and the other
and it has to encapsulate the rules the automatic rules that maintain that boundary in the face of a
of a changing environment could we spoke to um uh uh friston carl friston quite recently and
he was talking about mark of blankets which is quite interesting about how do you define the
the boundaries of a physical system and you know does a hurricane have a mark of boundary and
and what we're talking about here in a very general sense is defining boundaries between
what lives and what doesn't live and what is meaning and what isn't meaning and what is
understanding and what isn't understanding and it's very philosophical it's quite difficult
to pin this down if you look at uh maturana and virela's book it's about 60 pages the original
treatment auto breezes and cognition from from the 70s and it is very dense it's it's it's it's
it's not a woefully philosophical book it's quite hardcore and quite mathematical yeah i think they
they do do an interesting job at pinning down uh what uh if you like what it might be for something
to be alive um and this has been this challenge which was explored in in the embodied mind
has since been developed by evan thompson who's a uh an american feel american but uh
an interesting philosopher who wrote a book called mind in life where the argument is laid out
that there's a that life is a continuum and wherever you are at this continuum of life
then you have a proto mentality uh and i'm kind of drawn to that i think it's a very persuasive
argument and um then we have to look at the question of what constitutes autonomous systems
and what why should it matter if an autonomous system has a phenomenal sense of what it is like
to be um here if you like you can link into the work of a guy i've just recently come across
who wrote to me a few weeks ago uh he used to run a big lab in france a i lab in france
michael drwble i think his his name is and he he argues that we need phenomenal consciousness to
arbitrate between uh different actions if you're sending a robot to a mars it's absolutely
that's got to be completely autonomous and it's got to react appropriately in different sense and in
unknown environments to all sorts of different threats effectively the robot's got to have to know
something that it's a state that it's good to be that makes the robot feel pleasant and a state
that's horrible it's dangerous it might cause death to the robot it has to know that we use the
phenomenal sense of what that feels like we can then use that to arbitrate between different
actions and this is actually by the way is it an idea that was brought i first came across to a paper
by daniel denett called cognitive wheels the frame problem of ai when he looks at what must
be known to us to arbitrate on what you call the cookie problem imagine you've got a big jar of
cookies and some little kids like my six year old daughter and then two families one next to each other
and in one family when the child goes for a cookie the family beats it smacks it relentlessly until
it's in tears and never goes it doesn't have any more cookies after that and the other one they
they're very sort of touchy feeling oh no please don't have another cookie tarquin and occasionally
tarquin does go and have another cookie denett asks the question why is it that beating your kid
causes that child not to go for the cookie jar anymore and we know that well because being beaten
is something deeply unpleasant and uh you don't particularly unless you're a masochist want to
do things that are going to bring this feeling of pain about you but then denett said well
how do we know that we can we can we can arbitrarily hard wire six facts in so I could
hard wire into my computer program if something else biffs me then I'm going to increase my pain
by one and if pain gets over a certain threshold I wouldn't do that action again that's incredibly
brittle it's a little bit arbitrary but unless we have phenomenality unless we have access to
phenomenal states and know that getting biffed hurts or going over rough terrain if you're a
margin robot which takes you around a bit you you have to just sidestep that by hard wiring
effectively hard coding because it's engineers and the system is no longer autonomous now
we're having to define what it has to do for all these different possible states that's the price
we have to pay so I think you can argue that evolution is as as blestars with phenomenal
consciousness so that we can act autonomously and that's the way that I guess after Evan Thompson
and Varela and Tourboulay the view that I come to so we need consciousness to to enable us to
succeed evolutionarily well haven't we just fallen foul of the the kind of prime axiom of
software engineering at this point that you know every problem is just a level of abstraction
away because I mean we could you know um I forget which paper it was you had a really great uh
conclusion in one of your papers where you essentially said if we built a bunch of robots
that looked and behaved just like us they're automatically they they laugh at our jokes
they respond but the thing that defines like the difference here or the difference between us
and them would be that you know when when we're laughing and feeling we feel it it's phenomenological
when they do it it's not it's a simulation but you know leading into this argument it's like well
to have these martian robots that have autonomy and can succeed they need to have this phenomenological
state isn't this isn't this really just a software engineering problem away from being solved
I think when I wrote that paper I was I mean it's only very recently literally and I want to
I'm hoping to to work at least to reach out to Mikael to see if we can do something together so
I'm quite excited by these essays this sent me I think they make up as I don't know why I just
hadn't occurred to me that this could be a reason why consciousness has has evolved he makes a
very persuasive case I'd love to claim it as my idea but it absolutely isn't but I think it's a
very beautiful one I need to understand more and and either he will push it or perhaps we might do
something together I don't know um but yeah so I'm a little bit sceptical that without that the glue
of consciousness that we could get a machine to act as a similar chroma view in all possible cases
um I was arguing from the stronger I guess in that paper that well that's just assuming that we
can I think engineering wise I'm a little bit sceptical now following Mikael's work that that
is going to be possible but also to come back to another point uh that relates to the we're going
to talk about the Chinese room with all are you assuming that's boring for all your readers no no I
think that's one of the most important things because because which yeah we're talking now about
you know consciousness and the various different kind of boundaries between what is and what is not
consciousness but I think the other one it's really important is understanding and the boundaries
between what is and what isn't understanding you say in one of your papers what does it mean for a
central processing unit to understand does it understand the program and its variables in a
manner of analogous to cells understanding of this rulebook but this cells rulebook thing right it
describes a procedure as you say in one of your papers that if carried out accordingly um allows
cell to participate in an exchange of uninterrupted symbols squiggles and squiggles which to an outside
observer look as if so is accurately responding in Chinese to questions in Chinese about stories
in Chinese in other words it appears as if so in following his rulebook actually understands Chinese
even though so trenchantly continues to insist that he does not understand a word of the language so
this has been used I think very reliably and you had a paper actually recently introducing a couple
of other responses because there were four responses to cells argument right there was the robot reply
the systems reply the brain simulator reply the combination reply and in your recent paper you
talked about um robots and animats in the target bbs article there was a lot more than four so
when he wrote the paper came up with four possible counter arguments which are the classic ones
that tim just outlined but from memory there must have been over 20 people really big names in
philosophy and ai who responded to that bbs target article people like Marvin Minsky, McCarthy,
denner obviously uh god i can't i'm ashamed to say i've forgotten but they're
big big names uh who are different responses if there's a lot more than the four that's
so but i mentioned these because what's been bizarre having uh i edited today with John Preston
he edited um uh on the 20th 21st anniversary of the chinese room argument John Preston and I put
together an edited collection of of responses to it 20 year one years on from leading ai scientists
cognitive scientists and philosophers and i still think that's a good collection of essays
that we that we picked and good collection of people to contribute to that volume and um
again in the intervening years between this chinese room argument coming out and that volume
coming out and then between that volume coming out in 2002 and two and now really and i've talked
about this in as you can imagine in many places most of the uk universities and quite a few in
europe and one or two in america and nearly all the most formidable responses that i've come across
really go back to responses that Searle actually predicted and by far the most common and probably
the most i think the strongest responses is some variance on what became known as the system's
reply that you that you mentioned him can you just quickly define what the system's reply is
do you mind if i just just go over again as you went up really really quickly the essence of
the argument but i'd like just to sort of unpack it slightly more slowly so Searle imagine Searle to
set the scene Searle's a monoglot english speaker the only shamefully like myself shamefully because
i'm married to a greek lady i still can pretty well only communicate in english at best
and then Searle can as well so he imagines myself locked in a room and this room has got effectively
got a letter box instead of a door to which he could communicate with the outside world and in the
room of three piles of papers and on these papers are strange symbols that Searle doesn't know what
they are we know there's people reading about the experiment hearing about it that these are
actually chinese ideographs but to Searle they're just uninterpreted squiggles and squabbles you've
got no idea what they are so you've got these three piles of things and on the desk there's a big
grimoire a book that tells Searle how to correlate symbols from the first pile with symbols on the
second and then other rules that tell him how to correlate symbols on the first pile and with
the second pile and also linking symbols on the third pile and other rules that tell him how to
take symbols from one of these piles and stick them to people through their thoughts to the outside
world we'll unbeknownst to Searle the first pile defines a a script a second in chinese a second
pile describes a story in chinese and the third pile describes questions about that story in chinese
and the symbol Searle was told by the book to give to people in the outside world
answers to questions to that story in chinese and Searle's point is that if we concede so he's arguing
again from the stronger so it says okay let's concede but that rule book however it's defined and
again a lot of people got home because they thought Searle was purely talking about a naive pattern
matching programme if this symbol doesn't then do that actually Searle makes it clear if you read the
paper carefully that he wants us to stand for any conceivable computer programme this was the
first reaction that i had i had an allergic reaction to it because as we know from talking to
woolly tsubba you know that we can't write the damn compiler for the language it's too complicated so
um it it's not possible really for even us to explicitly understand and verbalise the rules that
we use in language and you said yourself that artificial intelligence practitioners were incredulous
at the extremely kind of you know simplistic view of of Searle that you could have this um you
know low level rules described yeah but i mean if that's because they didn't read the paper carefully
because Searle makes it absolutely explicit that he generalises he gives a simple example to
sort of just get you thinking about the problems as you imagine that's again so this is the world
that you know some people have tried to do language interpret understanding in this very naive
way but Searle wants the thought of the standard for any possible programme all we're doing is
meaning the rule book tells Searle how to manipulate uninterrupted symbols and put uninterrupted symbols
out of the door uh how it does that whether it's implementing a neural network whether it's
simply a genetic algorithm whether it's implementing woolly tsubba's uh sense-based uh uh uh word
effect sense-based worst effect compositional understanding that from language designing
whether it's doing uh uh GPT3 kind of operations it's irrelevant Searle says whatever your programme
is that's what's in the book and at the end of the day that programme will tell me how to respond
to questions in Chinese with answers in Chinese if I follow that programme uh carefully and don't
make any mistakes I'll give answers out the door if your programme's any good it will give answers
that are indistinguishable from those a natural and native speaking Chinese person would give
even though as Searle triumphantly insists I following this programme has not inevitably to get
even the toe hold in Chinese semantics all I've been doing is like a mega fast idiots of wrong
manipulating any sort of symbols around and sticking some symbols that I don't know what the
hell you are through a letterbox in and out to the outside world does that imply then that
it's just observationally impossible to determine whether a black box is conscious
well this is a I wrote a cancer argument to Susan Shryde I can't pronounce the name for Susan
Shryde is uh uh cheering test for machine consciousness why I make that exact claim
um you know uh this was in frontiers in the robotics one of the frontiers journals a couple
of years ago um because Susan says oh we can ask her ideas if we ask questions that are about
particularly human activities relating to phenomenal experience we'll be able to tell whether this
machine is uh she gives a procedure um for doing this I would say what if whatever set of
questions is a new have for deciding whether your machine is conscious I'm going to sit unbi
next year and watch you ask them I'm then going to go away and write a little programme in basic
because I'm good at writing in basic that says if question one says bladi bladi bladi are you
Susan's first question give this answer which is the answer that a really complicated machine
consciousness programme gave so I in fact have a look-up table but of course Susan doesn't know
now because I sneakily switched the machine so she asks her questions thinking she's talking
to a really complicated machine consciousness thing she asks the questions which she claims
will tell her whether this machine is conscious but she's actually just interrogating with a really
simple look-up thing and at the end it gives her the answers she wants as yeah that's conscious
but she's just been talking to a look-up table it's yeah I think that you're quite right Keith I think
isn't obvious to me how we're going to be able to do a test for machine consciousness purely on the
basis of external observation in the absence of anything else because we cannot if we're Machiavellian
we can always cheat the thing is when you were talking about it doesn't have semantics I want to
one pick that a little bit as well because you said the robot rover on Mars the semantics there
were there's the state spaces of all of the sensory experiences and then you said it's brittle
and there's an alignment problem and I can understand that it's very similar to the AI alignment argument
but this is different this if you if you can replicate let's say you're talking to a black box
and you can't distinguish whether or not it has consciousness or whether it has understanding
why is there an issue with semantics in that case? Ha you're wrong fitting it I thought you're
going to say how can I tell that anybody you understand is already conscious which is actually
one of the core responses that's all anticipated in the shiny zoom argument that that is actually
that is the obvious question as well like presumably you think that we are conscious
because we might exist in a computer simulation right well I don't think we can because I don't I know
that if I slap my face it hurts right and I know that machines can't instantiate phenomenal
I made a paper called a refuting digital ontology just after an invited talk at the
Royal Society workshop on the incomputable hosted by Barry Cooper who is the leader of the
Turing centenary celebrations in the UK and worldwide and I made this very argument then that
it isn't obvious to me I think that it's clearly obvious to me that we're not in a computer simulation
because I feel and if my dancing with pixies reduc your argument is correct unless I'm willing
to accept panthagism then computer computations can't realise sensation so either my dancing
with pixies reduc your is wrong in which case I'm very happy sad but in the sense I'll be happy
when you show me where I'm going wrong or if I'm right then we're not living in the computer
simulation so I don't think we are computer simulations furthermore it's an axiom of cognitive
science that whether minds exist you know so it's not for me to have to explain why I believe
that you three guys have phenomenal conscious states that's part of cognitive sciences is
acknowledging that you do and trying to come up with a theory that explains why these occur and
how they occur the conscious state argument to kind of perhaps play devil's advocate a bit here
to me when I first came across the dancing with pixies argument this wasn't an argument
that for me implied rocks had understanding or intelligence to me it implied that we don't
there's nothing that that has intelligence or understanding right what the intuition I'm
wanting you to come to is that computation doesn't have those phenomenal consciousness states I
actually the argument again interestingly what a reviewer said this of one version I think of the
2009 cognitive computing paper aren't you arguing too strongly I don't know doesn't this prove that
nothing can have consciousness no it doesn't it just says that the the operation of a digital
computer a finite state automata to be to pin it down more precisely cannot give rise to conscious
experience unless conscious experience is in everything that's all it says now if you don't
about the bullets it will actually I think I'm more than a finite state automaton thank you very
much Alex there's more to me I'm an embodied entity what I don't just think in my brain I think
with my body and my body in the world and again there's a beautiful result that came from a guy
and I'm going to argue this as a professor at Galsmys and I'm going to take his work and and
extend it probably in ways that he would he would be uncomfortable so this is my interpretation
of work that's published in a number of papers in nature by Jules Davidoff on colour perception
by the Himba tribe in his work in Africa I'll describe this beautiful experiment because
I would argue Jules is very cautious academic and he doesn't make wild proclamations that I'm
more comfortable doing the looking at his work but basically Jules did this beautiful
set of experiments over a long period of time working with the Himba tribe in African colour
perception and when I first saw them again they blew my mind because they basically
appeared to support the idea to me that language informs the not just the way that we package the
world we that we label it that's kind of obvious but the way that we see the world so how did Jules
his work show this well he took a series of colour slides like Munsell colour slides if you're
familiar with these well precise blocks of colour and on one piece of paper for argument sake there
were different shades of yellow and another one there for argument sake there were different
shades of green all equally different from one side of the next with the same colour difference
uniformly so they went from a second dark green if you like to a light green or whatever the
uniform colour differences needs at least one of the slides and on the sort of ochre yellowy one
if you showed that to Europeans said which is the odd one out if you looked really carefully
not always but often you'd say the one at the two o'clock position but it took you a lot of time
and you literally actually in your mind's eye compare all the slides with each other and if you
were lucky you said yeah it was that one but I've done this test and I mean sometimes I don't even see
myself it doesn't jump out it's not obvious it doesn't pop out and some of the times I make that
decision wrongly now you show that to the Himba and they sort of smile go bang if the two o'clock
one immediately now we call it places it leaps out of them and then we get the the green ones now
on the green ones there's a blue style so they show that to Europeans what's the odd one out
the blue one show that to the Himba and the guy's got a stick and he remember his head and
remember this imaginary beard and can't work it out now when you look at that from a
western linguistic perspective how can the person not see the blue one is the odd one out well it turns
transpired the Himba got very restricted or very limited colour vocabulary compared to your
hints and no doubt he took someone from the Himba and threw them into London after a period of time
there they would perceive the blue pop out you meet just as quickly as we do it's nothing to do
with the Himba's colour system it's an effect brought on by language but to me why what makes
this really powerful is it's a pop out effect right the Himba immediately get the one that's
the colour boundary that's important to them pops out to them because they've got a name for it and
it's a I can't remember what it was now whatever name it is an important thing and it pops out of
them straight away for us the blue green colour boundaries are brought and that pops out to us
straight away and that because it's a pop out effect that says that it's to me it's actually
what it is like to see the slide the Himba cannot possibly be seeing green and blue in the
way that I am otherwise the blue presumably would pop out of them because it they must be seeing
something bloody weird but not the they aren't seeing those green and blue tiles like I am
in their mind's eye for one of the better word and suddenly when we look at these sort of
oakery yellow ones they must be seeing that differently for me because I can't tell the odd
one out with our billion quickly in the way that they can now I think this gives us a very strong
evidence to me or at least this is the way that that I would like to use Jules's work and
I must underscore that I don't think he would be comfortable with any of this but this is the
way that I like to use it to say that you know that the language is actually affecting how we see
the world so when I say that our understanding of the world it's not just it's not just what's
going on in the brain the brain is instrumental to how we see and perceive and interact with the
world but it's not just that it's our entire body and also the body in the environment and also
within a social network of language users and all these things come together to enable us to form
perceptions of what it's like to see and what it's like to speak to the idea that we could just
reduce this to me a neural firing or even worse just some bloody manipulation of symbols by a
computer it's just ludicrous as far as I'm concerned I can see the the logical thought train here so
you're saying Jules said that language informs the way we see the world you know one potential
problem with that that's my interpretation on his work that's fine that's fine but I suppose
where I'm going with this is that it does it's starting to get towards this very kind of ultra
relativist constructivist view of the world and I want to put an anchor the reason why it's
interesting is when you're talking about sensory states that it doesn't actually seem like such
a bad thing because everything you're saying there completely makes sense of course depending
on where you are in the world and your language and so on you might have very very different
sensory states and you might experience color differently but then there's like a topology
isn't there so then you start getting into understanding and semantics right and then
you start getting into knowledge and truth and at some point you know people might start to
disagree with you that we should have a kind of ultra relativist view on what informs those things
so where do you draw the line then right well I'm one of the lovely things about being an academic
is that you get to work with some clever people I'm not clever but I've been blessed by working
with a number of people who bloody well are and at the moment I've got a postgraduate who's mind
boggling I like to say I supervise him either way around we're going for a discussion and I come
out feeling I've had my mind blown away and then he's sitting there looking quite chuffed with
how the way the decision's gone now his particular thesis has raised some really interesting
questions and he runs with his very postmodern ideas in extremis and I am on one particular
see for vision I'm diabetic and he was trying to make out so he could give me this argument
where my diabetic is just an enactment of a certain practice and I said that's nonsense I'll die
if I don't do this how can you decide you know the cell call thing the cell call conspiracy
or you know if I if I reap off its whole building I'm going to bloody well die it's not a social
concept for this but anyway parallel's work is is very very nuanced and he does make some radical
claims when you make radical claims you've got to be very very very careful about how you use
a language so I'm certainly not going to try and paraphrase his thesis in five minutes on here
because it's incredibly nuanced piece of work but I just want to take one aspect
from it that I think is interesting because I think it reflects on the world that we're living
in now and gives us an insight if it's possible insight into how trumpism and the echo chamber
culture has taken root because parallel and I'm not saying that you go along with this I'm just
trying to report this as best I can from our discussions parallel thinks that we don't just
have epistemic perspectives on a universally shared world where each of us ontologically can
have our own distinct ontologies and why does that matter well if it matters as parallel if you
start looking at how this is reflected on on Twitter if you have a community of people
and one of my other postcards a guy called christer mario did a film called right between
your ears which I can't recommend hardly enough looking at an end of the world documentary
full and documentary feature on on the end of the world cult on an end of the world cult in America
and how these guys were saying that ready in the bible was going to end on a certain day
and of course it didn't and how they then dealt with that and then the leaders are going to
end again in six months time yet again it didn't and Chris's film was just looking at how people
can arrive at these surreal beliefs and I think there's something akin to that with the q and on
movement and you had this idea that you know so my trump was going to lead and the result was
going to be overturned and it wasn't so well something that's going to happen if you're not
in that how can people have these bizarre ideas well how would I do that if you get in the community
of like-minded people if we go back to our basic philosophy and don't buy into a correspondence
theory of truth so much as a coherence theory of truth where a proposition is true because it
coheirs and with the body of other propositions that you take to be true when you're interacting
on a daily basis with people in your facebook bubble they will all share the same beliefs that
us outside that bubble look bloody bizarre they're reinforcing this thing so your view of the world
your ontological view of the world becomes really real you're drawn into this and that's why people
get so aggressive about it because it's not as an academic might abstractly discuss the truth or
falsity of I don't know Newton's Law's emotion or something this is what the world is you're
questioning something fundamental about these people's experience of the world we've we've kind
of covered a couple of big names tonight we've had Turing come up and Searle the other big name
that's kind of been floating in the background is Gerdl in in terms of like Gerdl Esherbach the the
famous book on human creativity and you kind of point to the Gerdelian we'll skip the other guys
but like the Gerdelian argument against the possibility of machine intelligence just for
for the guys out there that kind of want to get a bit of a grounding on why his name's been coming
up so much can you kind of just give us a very brief primer on how Gerdelian incomputability
kind of relates to the impossibility of machine intelligence and how that kind of ties into the
other arguments we've been hearing tonight yeah um I think in the architecture which you guys
I think hopefully signed posted at the beginning um artificial intelligence is stupid
there's a as best as I can give it a a one paragraph summary uh in mathematical form
of the Gerdelian argument and I don't think it'd be helpful to go through that line by line
now if your viewers are interested I refer them to the detail that's in that paper but to just
sort of talk at a slightly more abstract uh wider view um I think yeah I'm sure Gerdel and
Turing uh were both aware of the implications uh of their work on uh on logic in Gerdel's case
and on computability in Turing's case and in fact there's there are some people they think
for Turing and possibly for Gerdel as well but some people have made the claim that this
disconnect between Turing's avowedly physicalist to desire to explain all of human mentality
via a computer program plus what he'd learned professionally about the existence of non-computable
numbers led to a big tension in Turing's life um and there's a documentary uh that's looked at
dangerous ideas I think it was called the explored Gerdel Turing and Cantor in that context of people
who came up with ideas in their own work that challenged intuitions they had about the world
but coming off of Gerdel although I feel certain that Gerdel and Turing are both in their mind
gone through the implications of their work as far as I know one of the first times where this
this was actually cashed out in terms of an academic paper was by the Oxford philosopher John Lucas
in a series of pay exchanges with another philosopher called Paul Benekaraff in the 60s
where to cut to the chase I think that Lucas' argument is much more uh blunderbuss and
less sophisticated than Penrose's version of this so I commend anyone interested to look at
chatters of the mind rather than go back to Lucas but Lucas basically says for any any consistent
a mathematical system there will be sentences in that system that we outside of it can see
to be true the Gerdel sentence of that system but which we can prove can never be shown to be
true by that system unless the system is inconsistent and so that Lucas says you know
you give me any any version of computation that you like I can I as a human can step outside of
that and see things about that computational system that it provably cannot know for itself
and that's I think the first place where this argument sort of took off
um Penrose his own take on this is a little bit more on you want to see like to imagine looking
at in the shadows of the mind he looks at computations of one parameter and looks at the
question of whether that he looks at the halting problem effectively if you're familiar with that
on a function of one variable and that can you know is there a set of rules that will allow us
to deter to tell of any function of one parameter whether that function is computable or non computable
and he shows how that leads to a contradiction and if we follow the lines of the proof we see
that that comes to a point where we see a particular statement on the operation of computation k given
k is input we can see something about that computation that it cannot possibly terminate
that cannot be shown to be true following the rules of that system itself and I think that's an
interesting an argument and as I said I'm not it seems to me when I've read commentaries on
Penrose most people have criticised his speculations on quantum physics rather than what he has to
say about girdle and logic and in fact girdle and penrose was famously invited to give the keynote
at the centenary at the Vienna conference is honoring girdle for the centenary so I think
if there was some school boy error of course you occasionally meet was oh Penrose is wrong
and uh I think that Penrose is not stupid and I think if there was a school boy error in his work
and I'd like to think you'd have probably found it by now but I can recall when I was at the
University of Reading having an immensely long exchange with someone about Penrose's ideas
I won't name them but they were at the University of Sussex and after this had gone on for months and
months and months and I was saying look look at this look at this look at this paper and in the end
the guy turns back to you said life's too short to waste it reading what Penrose actually wrote
and I've come across this so often people think they know what Penrose has said
or they think they know that Silsred and they haven't bothered looking at the source material on
either and yeah this is an issue that comes up time and time again and that that was quite a shock
to me there when this guy didn't actually concede that he hadn't actually read Penrose at that point
in time I hope he has now but he hasn't fantastic well um professor bishop it's been an absolute
pleasure honestly thank you so much for for joining us today and um I hope to get you back on the show soon
well thank you yeah I really I can't I'd seem to be very rambly and unfocused description and
I'd try to think how you're going to get something interesting out of all this but that's your genius
behind the editing machine I feel yeah I hope it comes over okay no honestly it's absolutely
fascinating and I think we don't really have much of an opportunity to talk about philosophy of
mind to talk about some of these deeper issues in AI and I think this is a really fascinating
framework actually to think about some of the various different focus point I mean you know
we had Pedro Domingo's on last week talking about that that was awesome I really enjoyed that
that paper I read the paper that you foregrounded and also watched the little video that you did
accompanying it and um yeah I thought that was a really interesting uh deeply interesting paper
effectively you know I'm making the case that any that any gradient descent train neural networks
doing nothing more than a look at interplay between its training points effectively which
is a profound statement and uh I need to let the dust settle over that but it that was yeah
brilliant it's nothing else I mean yeah reading that was a great thing for the coding with you guys
it's good for deflating some of the deep learning hype to just to kind of like contextualise but
all like also with with regards to the night you've fleshed out probably 10 people's reading lists
or people's reading lists for the next sort of 10 years um I wouldn't worry too much about
rambling all that means that that is that there's a lot of ground to cover and not a lot of time
to cover it in this is the reason why I invited you on because a lot of these things are very
impenetrable and when I read your paper it's like a google maps kind of point by point stepping
stones list of all of the all of the important things that I need to know in order to you know
to get my head around this and I think it's actually a really great starting point for people
that are interested in philosophy of mind and understanding and consciousness and so on to
to read that paper that you wrote if I read one book on on modern cognitive science I'd say
look at Evan Thompson's mind in life it unfortunately it's a bloody fat book but it I think it's
that is a genius book but I just there's two watching a number of your podcasts now and I'm
really enthralled by all of them that
